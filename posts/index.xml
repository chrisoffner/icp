<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Chris Offner</title>
        <link>https://chrisoffner.github.io/studynotes/posts/</link>
        <description>Recent content in Posts on Chris Offner</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
        <lastBuildDate>Sun, 21 Nov 2021 19:08:00 +0100</lastBuildDate>
        <atom:link href="https://chrisoffner.github.io/studynotes/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Deriving the optimal ICP translation</title>
            <link>https://chrisoffner.github.io/studynotes/posts/2021/11/deriving-the-optimal-icp-translation/</link>
            <pubDate>Sun, 21 Nov 2021 19:08:00 +0100</pubDate>
            
            <guid>https://chrisoffner.github.io/studynotes/posts/2021/11/deriving-the-optimal-icp-translation/</guid>
            <description>In their paper On the ICP Algorithm the authors Esther Ezra, Micha Sharir, and Alon Efrat write:
 Lemma 2.3 At each iteration $i \ge 2$ of the algorithm, the relative translation vector $\Delta t_i$ satisfies
$$\boldsymbol{\Delta t_i} = \frac{1}{m} \sum_{\boldsymbol{a} \in A} \left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}})\right),$$
where $\boldsymbol{t_j} = \sum_{k=1}^j \boldsymbol{\Delta t_k}.$
Proof Follows using easy algebraic manipulations, based on the obvious equality that follows by construction</description>
            <content type="html"><![CDATA[<p>In their paper <a href="https://dl.acm.org/doi/10.1145/1137856.1137873"><strong>On the ICP Algorithm</strong></a> the authors <a href="https://www.cs.biu.ac.il/en/staff/113">Esther Ezra</a>, <a href="https://www.cs.tau.ac.il/~michas/">Micha Sharir</a>, and <a href="https://www2.cs.arizona.edu/~alon/">Alon Efrat</a> write:</p>
<blockquote>
<p><strong>Lemma 2.3</strong>
At each iteration $i \ge 2$ of the algorithm, the relative translation vector $\Delta t_i$ satisfies</p>
<p>$$\boldsymbol{\Delta t_i} = \frac{1}{m} \sum_{\boldsymbol{a} \in A} \left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}})\right),$$</p>
<p>where $\boldsymbol{t_j} = \sum_{k=1}^j \boldsymbol{\Delta t_k}.$</p>
<p><strong>Proof</strong>
Follows using easy algebraic manipulations, based on the obvious equality that follows by construction</p>
<p>$$\boldsymbol{\Delta t_i} = \frac{1}{m} \left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - (\boldsymbol{a} + \boldsymbol{t_{i-1}})\right)$$</p>
</blockquote>
<p>Unfortunately this supposedly <em>&ldquo;obvious equality&rdquo;</em>  wasn&rsquo;t quite so obvious to me, and neither were the &ldquo;<em>easy</em> algebraic manipulations&rdquo; that would prove the lemma.</p>
<p>Only after working through Cyrill Stachniss' great <a href="https://www.youtube.com/watch?v=dhzLQfDBx2Q"><em>lecture</em></a> on the topic was I able to derive this &ldquo;obvious&rdquo; equality, and then do the algebraic manipulations that would allow me to arrive at this lemma.</p>
<h2 id="getting-to-the-starting-point">Getting to the starting point</h2>
<h3 id="use-local-coordinate-system">Use local coordinate system</h3>
<p>To simplify notation we want to use the local coordinates defined by the point set $B$ and set the origin as the mean of $B$:</p>
<p>$$\boldsymbol{b_0} := \frac{1}{m} \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a + t_{i-1}})$$</p>
<p>Adding and subtracting this $\boldsymbol{b_0}$ to our $\mathrm{RMS}$ cost function does not change it, so that in every iteration $i$ we want to find the $\boldsymbol{\Delta t_i}$ that minimises</p>
<p>$$\mathrm{RMS}(\boldsymbol{\Delta t_i}) = \frac{1}{m}\sum_{a \in A} ||\boldsymbol{N_B(a+t_{i-1})} - \boldsymbol{b_0} - (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{\Delta t_i} + \boldsymbol{b_0}||^2.$$</p>
<h3 id="rewrite-translation-vector">Rewrite translation vector</h3>
<p>We introduce a <em><strong>new variable</strong></em> $\boldsymbol{a_0} := \boldsymbol{b_0} - \boldsymbol{\Delta t_i}$ and can then rewrite the expression of point $\boldsymbol{a}$ translated by the newest translation vector as</p>
<p>$$(\boldsymbol{a} + \boldsymbol{t_{i-1}}) + \boldsymbol{\Delta t_i} - \boldsymbol{b_0} = (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{a_0}.$$</p>
<p>Note that, while the notation of $\boldsymbol{a_0}$ alludes to a semantic meaning similar to that of $\boldsymbol{b_0}$, it is currently simply defined as $\boldsymbol{b_0} - \boldsymbol{\Delta t_i}$, with no obvious connection to the mean of $A$. The connection will show up as we proceed.</p>
<h3 id="minimisation">Minimisation</h3>
<p>Our initially formulated minimisation function</p>
<p>$$\frac{1}{m}\sum_{a \in A} ||\boldsymbol{N_B(a+t_{i-1})} - (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{\Delta t_i}||^2.$$</p>
<p>can therefore be rewritten with $\boldsymbol{a_0}$. Now in order to minimise our function we need to find</p>
<p>$$\argmin_{a_0}\frac{1}{m}\sum_{\boldsymbol{a} \in A} || (\boldsymbol{N_B(a+t_{i-1}}) - \boldsymbol{b_0}) - ((\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{a_0})||^2.$$</p>
<p>Let&rsquo;s simplify the notation in this long expression a bit by defining two new variables</p>
<p>$$\begin{aligned}\boldsymbol{\widetilde{b}} := &amp;N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) \cr \boldsymbol{\widetilde{a}} := &amp;\boldsymbol{a} + \boldsymbol{t_{i-1}}\end{aligned}$$</p>
<p>and use those as we expand our cost function:</p>
<p>$$\begin{aligned}\Phi(\boldsymbol{a_0}) \coloneqq \frac{1}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} &amp;\left[ (\boldsymbol{\widetilde{b}} - \boldsymbol{b_0}) - (\boldsymbol{\widetilde{a}} - \boldsymbol{a_0}) \right]^T\left[ (\boldsymbol{\widetilde{b}} - \boldsymbol{b_0}) - (\boldsymbol{\widetilde{a}} - \boldsymbol{a_0}) \right] \cr = \frac{1}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} &amp;(\boldsymbol{\widetilde{b}} - \boldsymbol{b_0})^T(\boldsymbol{\widetilde{b}} - \boldsymbol{b_0}) &amp;&amp; \text{(constant)} \cr + \frac{1}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} &amp;(\boldsymbol{\widetilde{a}} - \boldsymbol{a_0})^T(\boldsymbol{\widetilde{a}} - \boldsymbol{a_0})&amp;&amp;(*) \cr - \frac{2}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} &amp;(\boldsymbol{\widetilde{b}} - \boldsymbol{b_0})^T(\boldsymbol{\widetilde{a}} - \boldsymbol{a_0}) &amp;&amp; (**)\end{aligned}$$</p>
<p>To find the minimum of this convex function we calculate its first derivative</p>
<p>$$\begin{aligned}
\frac{\mathrm{d} \Phi}{\mathrm{d} \boldsymbol{a_0}} = &amp;-\frac{2}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}}(\boldsymbol{\widetilde{a}} - \boldsymbol{a_0})&amp;&amp;(*)\cr
&amp;+\frac{2}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} (\boldsymbol{\widetilde{b}} - \boldsymbol{b_0}) &amp;&amp; (**)
\end{aligned}$$</p>
<p>and set it equal to zero, which simplifies to</p>
<p>$$\sum_{a \in A}((\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{a_0}) = \sum_{a \in A} (\boldsymbol{N_B(a+t_{i-1}}) - \boldsymbol{b_0}).$$</p>
<p>The right side of this equation equals zero because $\boldsymbol{b_0}$ is the mean of $$ all nearest neighbours ${N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}},|, \boldsymbol{a} \in A)}$. Now we can solve for $\boldsymbol{a_0}$:</p>
<p>$$\begin{aligned}0 = &amp;\sum_{a \in A}((\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{a_0}) \cr = &amp;\sum_{a \in A} (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \sum a_0 \cr = &amp;\sum_{a \in A} (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - m \cdot \boldsymbol{a_0}\cr\implies \boldsymbol{a_0} = &amp;\frac{1}{m} \sum_{a \in A} (\boldsymbol{a} + \boldsymbol{t_{i-1}}) \end{aligned}$$</p>
<p>which means <strong>the optimal value for $\boldsymbol{a_0}$ is the weighted mean of the points</strong> $\boldsymbol{a} + \boldsymbol{t_{i-1}}$.</p>
<p>Since $\boldsymbol{a_0} = \boldsymbol{b_0} - \boldsymbol{\Delta t_i}$ and $\boldsymbol{b_0} = \frac{1}{m} \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a + t_{i-1}})$ it follows that</p>
<p>$$\begin{aligned}\boldsymbol{\Delta t_i} = &amp; \boldsymbol{b_0} - \boldsymbol{a_0} \cr = &amp;\frac{1}{m} \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a + t_{i-1}}) - \frac{1}{m} \sum_{\boldsymbol{a} \in A} (\boldsymbol{a} + \boldsymbol{t_{i-1}}) \cr = &amp;\frac{1}{m} \sum_{\boldsymbol{a} \in A} \left(N_B(\boldsymbol{a + t_{i-1}}) - (\boldsymbol{a} + \boldsymbol{t_{i-1}})\right) \end{aligned}$$</p>
<p>The optimal translation vector is therefore the mean of differences between points marked as nearest neighbours in $B$ and their corresponding points in $A + \boldsymbol{t_{i-1}}$.</p>
<h2 id="proving-the-lemma">Proving the lemma</h2>
<p>Using this equality we can finally perform the algebraic manipulation that leads us to the lemma, starting with a simple index shift:</p>
<p>$$\begin{aligned}
&amp;&amp;\boldsymbol{\Delta t_{i-1}} &amp;= \frac{1}{m} \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) - (\boldsymbol{a} + \boldsymbol{t_{i-2}}) \cr
\implies&amp;&amp;m\boldsymbol{\Delta t_{i-1}} &amp;=  \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) - \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-2}}) \cr\implies&amp;&amp;\sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) &amp;= \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-2}}) + m\boldsymbol{\Delta t_{i-1}}\cr\implies&amp;&amp;\sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) &amp;= \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-2}}) + \sum_{\boldsymbol{a} \in A}\boldsymbol{\Delta t_{i-1}}\cr\implies&amp;&amp;\sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) &amp;= \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-2}} + \boldsymbol{\Delta t_{i-1}})\cr\implies&amp;&amp;\sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) &amp;= \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-1}})\cr\implies&amp;&amp;\frac{1}{m}\sum_{\boldsymbol{a} \in A} \left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}})\right) &amp;= \underbrace{\frac{1}{m}\sum_{\boldsymbol{a} \in A}\left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) -(\boldsymbol{a} + \boldsymbol{t_{i-1}})\right)}_{=\boldsymbol{\Delta t_i}}\end{aligned}$$</p>
<p>Thanks to the previously derived equality we know the right side is $\boldsymbol{\Delta t_i}$, and can therefore confirm that the optimal relative translation for every iteration $i \ge 2$ is the mean difference between the current and previous nearest neighbour of each point in $A$.</p>
]]></content>
        </item>
        
        <item>
            <title>Illustrating ICP in one dimension</title>
            <link>https://chrisoffner.github.io/studynotes/posts/2021/11/illustrating-icp-in-one-dimension/</link>
            <pubDate>Sat, 20 Nov 2021 16:35:48 +0100</pubDate>
            
            <guid>https://chrisoffner.github.io/studynotes/posts/2021/11/illustrating-icp-in-one-dimension/</guid>
            <description>To get a better intuition of what the iterative closest point (ICP) algorithm does, let&amp;rsquo;s look at a visual representation of its simplest form – the one-dimensional case which is only concerned with translation, but not rotation.
Problem definition Let $A = \{{\boldsymbol{a_1}, &amp;hellip;, \boldsymbol{a_m}\}}, B = \{{\boldsymbol{b_1}, &amp;hellip;, \boldsymbol{b_n} \}}$ be two point sets in $d$-space, in the following illustration $d = 1$, and suppose that the ICP algorithm aligns $A$ to $B$, i.</description>
            <content type="html"><![CDATA[<p>To get a better intuition of what the iterative closest point (ICP) algorithm does, let&rsquo;s look at a visual representation of its simplest form – the one-dimensional case which is only concerned with translation, but not rotation.</p>
<h3 id="problem-definition">Problem definition</h3>
<p>Let $A = \{{\boldsymbol{a_1}, &hellip;, \boldsymbol{a_m}\}}, B = \{{\boldsymbol{b_1}, &hellip;, \boldsymbol{b_n} \}}$ be two point sets in $d$-space, in the following illustration $d = 1$, and suppose that the ICP algorithm aligns $A$ to $B$, i.e. $B$ is fixed and $A$ is translated to best fit $B$.</p>
<p>The algorithm repeatedly iterates over its two steps:</p>
<ol>
<li>
<p>For each (translated) point $\boldsymbol{a} + \boldsymbol{t_{i-1}} \ in A + \boldsymbol{t_{i-1}}$ it registers a correspondence to its nearest neighbour $N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}})$ in $B$.</p>
</li>
<li>
<p>Based on these point correspondences, it then computes a relative translation vector $\boldsymbol{\Delta t_i}$ that minimises a cost function $\Phi$ and translates $A + \boldsymbol{t_{i-1}}$ by that vector.</p>
</li>
</ol>
<p>The algorithm terminates once $\boldsymbol{\Delta t_i} = 0$.</p>
<h3 id="illustration">Illustration</h3>
<p>Let&rsquo;s now look at a visual representation of a full run of the ICP algorithm in one dimension. For better illustration, the point sets $A$ (red) and $B$ (blue) have been offset vertically, but are to be understood as both lying on the same one-dimensional number line $\R$. The following example uses the <em><a href="https://en.wikipedia.org/wiki/Root_mean_square">root mean square</a></em> cost function</p>
<p>$$\Phi(\boldsymbol{\Delta t_i}) = \mathrm{RMS}(\boldsymbol{\Delta t_i}) = \frac{1}{m}\sum_{a \in A}|| (a + t_i + \boldsymbol{\Delta t_i}) - N_B(a + t_{i-1}) ||^2$$</p>
<p>where $||\cdot||$ is the Euclidean norm and $m = |A|$.</p>
<p>The notation largely follows the conventions laid out in <em><a href="https://dl.acm.org/doi/10.1145/1137856.1137873">On the ICP Algorithm</a></em> by Esther Ezra, Micha Sharir, and Alon Efrat.</p>
<h4 id="iteration-1">Iteration 1</h4>
<p>We begin with $A = \{{ 16, 17, 25, 26 \}}$ and $B = \{{1, 2, 10, 11\}}$. It is easy to see that in this first iteration, all points of $A$ will be assigned $\boldsymbol{b_4} = 11$ as their nearest neighbour, and will therefore be &ldquo;pulled&rdquo; strongly to the left.</p>
<p><img src="/images/icp_graph_01.svg" alt="icp_graph_01.svg"></p>
<h4 id="iteration-2">Iteration 2</h4>
<p>After the first translation by $\boldsymbol{\Delta t_1} = -10$, something interesting happens. The two left-most points in the now translated red set, $A + \boldsymbol{t_1}$, have moved so far that they get assigned new nearest neighbours. Now the second red point $a_2 +  \boldsymbol{t_1}$ pulls to its right, and the resulting optimal translation vector $\boldsymbol{\Delta t_2} = -2.5$ is already much shorter than before.</p>
<p><img src="/images/icp_graph_02.svg" alt="icp_graph_02.svg"></p>
<h4 id="iteration-3">Iteration 3</h4>
<p>In the next iteration, the second red point&rsquo;s corresponding point changes again, causing it to join the others again in &ldquo;pulling&rdquo; further to the left.</p>
<p><img src="/images/icp_graph_03.svg" alt="icp_graph_03.svg"></p>
<h4 id="iteration-4">Iteration 4</h4>
<p>Now the first and third red point get new nearest neighbours in $B$, resulting in one final slight tug to the left.</p>
<p><img src="/images/icp_graph_04.svg" alt="icp_graph_04.svg"></p>
<h3 id="iteration-5">Iteration 5</h3>
<p>In the end, $A + \boldsymbol{t_4}$ rests in the optimal position so that $\boldsymbol{\Delta t_5} = 0$ and the algorithm terminates.</p>
<p><img src="/images/icp_graph_05.svg" alt="icp_graph_05.svg"></p>
<p>Next up we will derive the optimal relative translation vector $\boldsymbol{\Delta t_i}$ at every step.</p>
]]></content>
        </item>
        
    </channel>
</rss>
