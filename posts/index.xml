<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Chris Offner</title>
        <link>https://chrisoffner.github.io/studynotes/posts/</link>
        <description>Recent content in Posts on Chris Offner</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
        <lastBuildDate>Sun, 12 Dec 2021 10:58:57 +0100</lastBuildDate>
        <atom:link href="https://chrisoffner.github.io/studynotes/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Proving the lower bound for ICP 1</title>
            <link>https://chrisoffner.github.io/studynotes/posts/2021/12/proving-the-lower-bound-for-icp-1/</link>
            <pubDate>Sun, 12 Dec 2021 10:58:57 +0100</pubDate>
            
            <guid>https://chrisoffner.github.io/studynotes/posts/2021/12/proving-the-lower-bound-for-icp-1/</guid>
            <description>‚ö†Ô∏è Disclaimer ‚ö†Ô∏è These are my study notes for theorem 4.1 of On the performance of the ICP algorithm by Ezra et al. in which I try to retrace their proof.
These notes are still very much incomplete and a work in progress.
ü§î Question about theorem 4.1 ü§î   ü§î Question about what seems like a contradiction ü§î   Round $j$ ist the sequence of all iterations, in which any points of $A$ cross the common Voronoi boundary $\beta_{n-j+1}$ of the cells $\mathcal{V}(b_{n-j+1}), \mathcal{V}(b_{n-j+2})$.</description>
            <content type="html"><![CDATA[<h2 id="-disclaimer-">‚ö†Ô∏è Disclaimer ‚ö†Ô∏è</h2>
<p>These are my study notes for theorem 4.1 of <a href="https://www.sciencedirect.com/science/article/pii/S0925772108000217">On the performance of the ICP algorithm</a> by Ezra et al. in which I try to retrace their proof.</p>
<p>These notes are still very much incomplete and a work in progress.</p>
<h3 id="-question-about-theorem-41-">ü§î Question about theorem 4.1 ü§î</h3>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/aA4yMXN8SG4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h3 id="-question-about-what-seems-like-a-contradiction-">ü§î Question about what seems like a contradiction ü§î</h3>
<ul>
<li>
<p>Round $j$ ist the sequence of all iterations, in which any points of $A$ cross the common Voronoi boundary $\beta_{n-j+1}$ of the cells $\mathcal{V}(b_{n-j+1}), \mathcal{V}(b_{n-j+2})$.</p>
</li>
<li>
<p>$\boldsymbol{(ii)}:$ In each iteration in round $j$, other than the last one, the overall number of points of $A$ that cross $\beta_{n-j+1}$ is exactly $j$, and no point crosses any other boundary</p>
</li>
<li>
<p>$\boldsymbol{(iii)}:$ In the last iteration of round $j$, the overall number of points of $A$ that cross either $\beta_{n-j+1}$ or  $\beta_{n-j+2}$ is exactly $j-1$.</p>
</li>
<li>
<p>In the last iteration of round $j$, all the points of $A' := \{a_2, &hellip;, a_n\}$, except $l$ of them, have crossed $\beta_{n-j+1}$ in previous iterations.</p>
</li>
<li>
<p>So in this last iteration of round $j$, exactly $l &gt; 0$ points need to cross $\beta_{n-j+1}$, otherwise it wouldn&rsquo;t be the last iteration of round $j$, and exactly $j - 1$ points cross <em>any</em> boundary.</p>
</li>
<li>
<p>If in this last iteration of round $j$, any points cross $\beta_{n-j+2}$, this iteration is then also the <em>first iteration</em> of the next round, $j - 1$.</p>
</li>
<li>
<p>$\boldsymbol{(ii)}$ demands that means exactly $j-1$ points need to cross $\beta_{n-j+2}$ in this iteration.</p>
</li>
<li>
<p>But that leaves none of the $j-1$ possible boundary crossings for the $l$ points supposedly still on the left of $\beta_{n-j+1}$, i.e. only $l = 0$ is possible.</p>
</li>
<li>
<p>Isn&rsquo;t this a contradiction? ‚ö°Ô∏è</p>
</li>
</ul>
<hr>
<h2 id="which-points-contribute-to-the-translation">Which points contribute to the translation?</h2>
<p>$$\boldsymbol{\Delta t_i} = \frac{1}{m} \sum_{a \in A} (N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - N_B(\boldsymbol{a} +\boldsymbol{t_{i-2}}))$$ implies that any point $\boldsymbol{a}$ that did not change its corresponding nearest neighbour after the previous translation,  i.e. all $a$ for which $N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) =  N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}})$, does not contribute to $\boldsymbol{\Delta t_i}$.</p>
<p>The algorithm terminates once no point $\boldsymbol{a}$ has changed its nearest neighbour, in which case $\boldsymbol{\Delta t_i} = \boldsymbol{0}$.</p>
<p>üí° <em><strong>In each iteration, only the points that changed their neighbour during the previous translation contribute to the translation.</strong></em> üí°</p>
<hr>
<h2 id="theorem-41">Theorem 4.1</h2>
<p>We construct two point sets $A, B$ on the real line, where $|A| = |B| = n$.</p>
<p>$A$ consists of the points $a_1 &lt; &hellip; &lt; a_n$, where $$\begin{aligned}a_1 &amp;= - n - (n-1)\delta,\cr a_i &amp;= \frac{2(i-1) - n}{2n} + \delta = \frac{b_i}{n} - \frac{1}{2} + \delta,\end{aligned}$$ for $i=2,&hellip;,n$, and $\delta = o(1/n)$ is some sufficiently small parameter.</p>
<p>$B$ consists of the points $b_i = i-1$, for $i=1,&hellip;,n$.</p>
<p>Initially, all points of $A$ are assigned to $b_1$. As the algorithm progresses, it keeps translating $A$ to the right.</p>
<p>Since we mostly ignore the point $a_1$ for this proof, we define $A' := A \backslash \{a_1\} = \{a_2, &hellip;, a_n\}$.</p>
<p>The first translation satisfies $\Delta t_1 = 1$ (üßÆ<a href="#first-translation--boldsymboldelta-t_1">computation</a>), which implies that after the first iteration of the algorithm, all points of $A'$ are assigned to $b_2$.</p>
<p>Using</p>
<p>$$\Delta t_i = \frac{1}{m} \sum_{a\in A}\left(N_B(a + t_{i-1}) - N_B(a + t_{i-2})\right), \qquad(2)$$</p>
<p>we have</p>
<p>$$\Delta t_2 = \frac{1}{n} \left(\cancel{b_1} - \cancel{b_1} + \sum_{i=2}^{n} (b_2 - b_1)\right) = \frac{n-1}{n}$$</p>
<p>which implies that the $n-1$ points of $A'$ move to the next Voronoi cell $\mathcal{V}(b_3)$ after the second iteration, so that the distance between the new position of $a_n$ from the right boundary of $\mathcal{V}(b_3)$ is $\frac{2}{n} - \delta$, and the distance between the new position of $a_2$ and the left boundary of $\mathcal{V}(b_3)$ is $\delta$, as is easily verified:</p>
<p>$$\begin{aligned}
a_i + \Delta t_1 + \Delta t_2 &amp;= \frac{2(i-1) - n}{2n} + \delta + 1 + \frac{n-1}{n} \cr
b_3 = 2 &amp;\implies \begin{cases}\inf\mathcal{V}(b_3) = 1.5, &amp;&amp; \text{(left cell boundary)}\cr \sup\mathcal{V}(b_3) = 2.5 &amp;&amp; \text{(right cell boundary)}\end{cases}
\end{aligned}$$</p>
<p>The distance of $a_2$ from the left cell boundary of $\mathcal{V}(b_3)$ is $\delta$ (üßÆ<a href="#-distance-of--boldsymbola_2--t_2-from-the-left-boundary-of--boldsymbolmathcalvb_3">computation</a>).</p>
<p>The distance of $a_n$ from the right cell boundary of $\mathcal{V}(b_3)$ is $\frac{2}{n} - \delta$ (üßÆ<a href="#-distance-of--boldsymbola_n--t_2-from-the-right-boundary-of--boldsymbolmathcalvb_3">computation</a>).</p>
<p>Thus, after translating by $t_2$, all points of $A'$ are in $\mathcal{V}(b_3)$.</p>
<p>In the next iteration, $\Delta t_3 = \frac{n-1}{n}$ (arguing as above).</p>
<p>However, due to the current position of the points of $A$ in $\mathcal{V}(b_3),$ only the $n - 2$ rightmost points of $A$ cross the right boundary of $\mathcal{V}(b_3)$ into $\mathcal{V}(b_4)$, and the nearest neighbor of $a_2$ remains unchanged (equal to $b_3$).</p>
<p><strong>$\boldsymbol{a_2}$ doesn&rsquo;t cross into $\mathcal{V}(b_4)$:</strong></p>
<p>$$\begin{aligned}
a_2 + t_3 &amp;= a_2 + \Delta t_1 + \Delta t_2 + \Delta t_3 \cr
&amp;=\frac{2(2-1) - n}{2n} + \delta + 1 + 2\frac{n-1}{n}\cr
&amp;=\frac{1}{n} - \frac{1}{2} + \delta + 1 + 2\frac{n-1}{n}\cr
&amp;= \frac{2n-1}{n} + \frac{1}{2} + \delta\cr
&amp;= 2.5 - \frac{1}{n} + \delta\cr
&amp;&lt; 2.5 = \inf\mathcal{V}(b_4)
\end{aligned}$$</p>
<p>We next show, using the induction on the number of Voronoi cells the points of $A$ have crossed so far, the following property.</p>
<p>Assume that the points of $A'$ are assigned to $b_{n-j+1}$ and $b_{n-j+2}$, for some $1 \le j \le n$.</p>
<blockquote>
<p>‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è</p>
<p>For $j=1$ the point $b_{n-j+2} = b_{n+1}$ does not exist!</p>
<p>Shouldn&rsquo;t it be $2 \le j \le n$?</p>
<p>‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è</p>
</blockquote>
<p>Clearly, these assignments can involve only two consecutive Voronoi cells.</p>
<p>Consider all iterations of the algorithm, in which some points of $A$ cross the common Voronoi boundary $\beta_{n-j+1}$ of the cells $\mathcal{V}(b_{n-j+1}), \mathcal{V}(b_{n-j+2})$.</p>
<p>We call the <em>sequence</em> of these iterations <strong>round</strong> $\boldsymbol{j}$ of the algorithm.</p>
<p>Then</p>
<ul>
<li>$\boldsymbol{(i)}$ at each such iteration the relative translation is $\frac{j}{n}$,</li>
<li>$\boldsymbol{(ii)}$ at each iteration in this round, other than the last one, the overall number of points of $A$ that cross $\beta_{n-j+1}$ is exactly $j$, and no point crosses any other boundary, and</li>
<li>$\boldsymbol{(iii)}$ at the last iteration of the round, the overall number of points of $A$ that cross either $\beta_{n-j+1}$ or  $\beta_{n-j+2}$ is exactly $j-1$.</li>
</ul>
<p>In fact, in the induction step we assume that properties $\boldsymbol{(i)}, \boldsymbol{(ii)}$ hold, and then show that property $\boldsymbol{(iii)}$ follows, for $j$, and that $\boldsymbol{(i)}$ and $\boldsymbol{(ii)}$ hold for $j-1$.</p>
<p>To prove this property, we first note, using $$\boldsymbol{\Delta t_i} = \frac{1}{n} \underbrace{\sum_{a \in A} (N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - N_B(\boldsymbol{a} +\boldsymbol{t_{i-2}}))}_{k},\qquad(2)$$ that the relative translation at each iteration of the algorithm is $\frac{k}{n}$, where for iterations $i \ge 2$, $k$ is an integer $1 \le k \le n$ equal to the number of points in $A$ that have changed their nearest neighbour in the last iteration, and $k=n$ for $i=1$.</p>
<p>The preceding discussion shows that the induction hypothesis holds for $j = n$ and $j = n - 1$.</p>
<p><strong>Let&rsquo;s go over it explicitly though.</strong></p>
<h2 id="base-cases">Base cases</h2>
<hr>
<h3 id="case-boldsymbolj--n">Case $\,\boldsymbol{j = n}$</h3>
<p>Round $j = n$ consists of all iterations in which points of $A$ cross the voronoi boundary $\beta_{1}$.</p>
<p>We have shown that $\Delta t_1 = 1 = \frac{n}{n}$ moves the points $a_2, &hellip;, a_n$ across $\beta_{1}$.</p>
<p>Round $j = n$ therefore consists of only a single iteration ‚Äì the first one.</p>
<blockquote>
<p><em>$\boldsymbol{(i)}$ At each iteration of round $j$ the relative translation is $\frac{j}{n}.$</em></p>
</blockquote>
<p>This holds because $\Delta t_1 = \frac{n}{n} = \frac{j}{n}$.</p>
<blockquote>
<p><em>$\boldsymbol{(ii)}$ At each iteration in round $j$, other than the last one, the overall number of points of $A$ that cross $\beta_{n-j+1}$ is exactly $j$, and no point crosses any other boundary.</em></p>
</blockquote>
<p>$\boldsymbol{(ii)}$ is trivially true for round $j = n$, since the one and only iteration in this round is of course also its last one, and $(ii)$ makes statements only on iterations <em>other than the last one</em> in a round.</p>
<hr>
<h3 id="case-boldsymbolj--n---1">Case $\,\boldsymbol{j = n - 1}$</h3>
<p>Round $j = n - 1$ consists of all iterations in which points of $A$ cross the voronoi boundary $\beta_{2}$.</p>
<p>We have shown that $\Delta t_2 = \frac{n - 1}{n}$ moves the points $a_2, &hellip;, a_n$ across $\beta_{2}$.</p>
<p>Round $j = n - 1$ therefore also consists of only a single iteration ‚Äì the second one.</p>
<blockquote>
<p><em>$\boldsymbol{(i)}$ At each iteration of round $j$ the relative translation is $\frac{j}{n}.$</em></p>
</blockquote>
<p>This holds because $\Delta t_2 = \frac{n - 1}{n} = \frac{j}{n}$.</p>
<blockquote>
<p><em>$\boldsymbol{(ii)}$ At each iteration in round $j$, other than the last one, the overall number of points of $A$ that cross $\beta_{n-j+1}$ is exactly $j$, and no point crosses any other boundary.</em> {#computation1}</p>
</blockquote>
<p>As in the previous case $j = n$, claim $\boldsymbol{(ii)}$ is trivially true because round $j = n - 1$ consists of only a single iteration.</p>
<hr>
<h2 id="induction-step">Induction step</h2>
<p>Suppose that the induction hypothesis holds for all $j' \ge j$, for some $2 \le j \le n-1$, and consider round $j-1$ of the algorithm, i.e. the sequence of iterations during which points of $A$ cross $\beta_{n-j+2}$.</p>
<p>According to $\boldsymbol{(ii)}$ at each iteration of round $j$ (except for the last one), in which there are points of $A$ that remain in the cell $\mathcal{V}(b_{n-j+1})$, the $j$ rightmost points of $A$ (among those contained in $\mathcal{V}(b_{n-j+1})$) cross $\beta_{n-j+1}$.</p>
<p>Let us now consider the last such iteration. In this case, all the points of $A$, except $l$ of them, for some $0 \le l &lt; j$ (and $a_1$, which we ignore), have crossed $\beta_{n-j+1}$ in previous iterations.</p>
<blockquote>
<p>‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è</p>
<p><strong>How do we know that $\boldsymbol{l &lt; j}$?</strong></p>
<p>I understand that $l \le j$ because we&rsquo;re in the last iteration of round $j$, in which all $l$ points need to cross $\beta_{n-j+1}$, and we&rsquo;re translating by $\frac{j}{n}$, with which we can move up to $j$ points across $\beta_{n-j+1}$, but no more than $j$ points.</p>
<p>But why can&rsquo;t $l = j$?</p>
<p>‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è</p>
</blockquote>
<p>The key observation is that the distance from the current position of $a_n$ to the next Voronoi boundary $\beta_{n-j+2}$ is $\frac{l + 2}{n} - \delta$.</p>
<p>This follows since we shift in total $n-1$ points of $A$ that are equally spaced apart by $\frac{1}{n}$.</p>
<blockquote>
<p>üí°üí°üí°</p>
<p>Each voronoi cell is an interval of length $1$.</p>
<p>The $n-1$ points of $A'$ span an interval of $\frac{n-2}{n} &lt; 1$, which means that $a_2$ and $a_n$ can be either in the same voronoi cell, or in consecutive voronoi cells, but they cannot be separated by an entire cell.</p>
<p>Earlier we have shown that $a_2 + t_2$ is $\delta$ away from the boundary to its left, and that we always move by integer multiples of the distance $\frac{1}{n}$ between consecutive points of $A'$.</p>
<p>This means that in the last iteration of a round, the leftmost point that has crossed the boundary in the previous iteration, $a_{l+2}$, is always exactly $\delta$ away from its nearest left boundary.</p>
<p>If there are $l$ of the $n-1$ points of $A'$ remaining in $\mathcal{V}(b_{n-j+1})$, then there are $n-1 - l$ points already in $\mathcal{V}(b_{n-j+2})$.</p>
<p>$a_n$ is therefore $(n-2-l) \cdot \frac{1}{n} = 1-\frac{l + 2}{n}$ to the right of $a_{l+2}$.</p>
<p>Since $a_{l+2}$ is $\delta$ to the right of $\beta_{n-j+1}$, and |$\mathcal{V}(b_{n-j+2})| = 1$, that means $a_n$ is $1 - \left(1- \frac{l + 2}{n}\right) - \delta = \frac{l+2}{n} - \delta$ away from $\mathcal{V}(b_{n-j+2})$.</p>
<p>üí°üí°üí°</p>
</blockquote>
<hr>
<p>Since the next translation satisfies $\Delta t = \frac{j}{n}$ (using the induction hypothesis and $$\Delta t_i = \frac{1}{n} \sum_{a \in A} (N_B(a + t_{i-1}) - N_B(a + t_{i-2}))\qquad(2)$$), it follows that only $j-1$ points of $A$ cross a Voronoi boundary in the next iteration.</p>
<p>Moreover, the points $a_2, &hellip;, a_{l+1}$ cross the boundary $\beta_{n-j+1}$, and the points $a_{n-(j-l-2)}, &hellip;, a_n$ cross the boundary $\beta_{n-j+2}$ (this is the first move in which this boundary is crossed at all).</p>
<p>Thus, at the next iteration, since only $j-1$ points have just crossed between Voronoi cells, (2) implies that the next translation is $\frac{j-1}{n}$, and, as is easily verified, at each further iteration, as long as there are at least $j-1$ points of $A$ to the left of $\beta_{n-j+2}$, this property must continue to hold, and thus $j-1$ points will cross $\beta_{n-j+2}$. This establishes the induction step.</p>
<p>It now follows, using the above properties, that the number of iterations required for all the points of $A$ to cross $\beta_{n-j+1}$ is $\lceil\frac{n}{j}\rceil$ (‚ÅâÔ∏è shouldn&rsquo;t this be $\lceil\frac{n-1}{j}\rceil$, to be more precise ‚ÅâÔ∏è), where in the first (last) such iteration some of the points may cross $\beta_{n-j}$ ($\beta_{n-j+2}$) as well.</p>
<p>This implies that the number of such iterations, in which the points of $A$ cross only $\beta_{n-j+1}$ (and none of the two neighboring Voronoi boundaries), is at least $\lceil\frac{n}{j}\rceil - 2$ (but no more than $\lceil\frac{n}{j}\rceil$).</p>
<p>Thus the overall number of iterations of the algorithm is $\Theta\left(\sum_{j=1}^n \lceil\frac{n}{j}\rceil\right) = \Theta(n\log n)$. $\quad\square$</p>
<hr>
<h2 id="computation">Computation</h2>
<h3 id="-first-translation--boldsymboldelta-t_1">üßÆ First translation $\, \boldsymbol{\Delta t_1}$</h3>
<p>$$\begin{aligned}
\Delta t_1 &amp;= \frac{1}{n} \sum_{i=1}^n (N_B(a_i + t_{i-1}) - (a_i + t_{i-1})) \cr
&amp;= \frac{1}{n} \sum_{i=1}^n (b_1 - a_i) \cr
&amp;= \frac{1}{n} \left(b_1 - a_1 + \sum_{i=2}^n (b_1 - a_i)\right) \cr
&amp;= \frac{1}{n} \left( n + (n-1)\delta - \sum_{i=2}^n \left(\frac{2(i-1)-n}{2n} +\delta\right) \right) \cr
&amp;= \frac{1}{n} \left( n + \cancel{(n-1)\delta} - \cancel{(n-1)\delta} - \sum_{i=2}^n \left(\frac{2(i-1)-n}{2n} \right) \right) \cr
&amp;= \frac{1}{n} \left(n - \sum_{i=2}^n \left(\frac{i-1}{n} - \frac{1}{2} \right) \right) \cr
&amp;= \frac{1}{n} \left(n + \frac{n-1}{2} -\frac{1}{n}\sum_{i=2}^n (i-1) \right) \cr
&amp;= \frac{1}{n} \left(n + \frac{n-1}{2} -\frac{1}{n}\sum_{i=1}^{n-1} i \right) \cr
&amp;= \frac{1}{n} \left(n + \frac{n-1}{2} -\frac{1}{2n} (n-1)n \right) \cr
&amp;= \frac{1}{n} \left(n + \cancel{\frac{n-1}{2}} -\cancel{\frac{n-1}{2}} \right) \cr
&amp;= \frac{1}{n} \left(n\right) = 1
\end{aligned}$$</p>
<hr>
<h3 id="-distance-of--boldsymbola_2--t_2-from-the-left-boundary-of--boldsymbolmathcalvb_3">üßÆ Distance of $\, \boldsymbol{a_2 + t_2}\,$ from the left boundary of $\, \boldsymbol{\mathcal{V}(b_3)}$</h3>
<p>$$\begin{aligned}
(a_2 + \boldsymbol{t}_2) - \inf\mathcal{V}(b_3) = &amp;\frac{2(2-1) - n}{2n} + \delta + 1 + \frac{n-1}{n} - 1.5 \cr
= &amp;\frac{2 - n}{2n} + \frac{n-1}{n} + \delta - 0.5 \cr
= &amp;\frac{2 - n + 2(n-1)}{2n} + \delta - 0.5 \cr
= &amp;\frac{1}{2} + \delta - 0.5 \cr
= &amp;\delta \cr\cr
\implies &amp;a_2 \in \mathcal{V}(b_3)
\end{aligned}$$</p>
<hr>
<h3 id="-distance-of--boldsymbola_n--t_2-from-the-right-boundary-of--boldsymbolmathcalvb_3">üßÆ Distance of $\, \boldsymbol{a_n + t_2}$ from the right boundary of $\, \boldsymbol{\mathcal{V}(b_3)}$</h3>
<p>$$\begin{aligned}
\sup\mathcal{V}(b_3) - (a_n + \boldsymbol{t}_2) = &amp;2.5 - \left(\frac{2(n-1) - n}{2n} + \delta + 1 + \frac{n-1}{n}\right) \cr
= &amp;2.5 - \frac{2(n-1) - n}{2n} - \delta - 1 - \frac{n-1}{n} \cr
= &amp;1.5 - \left(\frac{n-1}{n} - \frac{1}{2}\right) - \delta - \frac{n-1}{n} \cr
= &amp;2 - \frac{n-1}{n} - \delta - \frac{n-1}{n} \cr
= &amp;2 - 2\frac{n-1}{n} - \delta\cr
= &amp;2\left(1 - \frac{n-1}{n}\right) - \delta\cr
= &amp;\frac{2}{n} - \delta\cr\cr
\implies &amp;a_n \in \mathcal{V}(b_3)
\end{aligned}$$</p>
<hr>
]]></content>
        </item>
        
        <item>
            <title>Deriving the optimal ICP translation</title>
            <link>https://chrisoffner.github.io/studynotes/posts/2021/11/deriving-the-optimal-icp-translation/</link>
            <pubDate>Sun, 21 Nov 2021 19:08:00 +0100</pubDate>
            
            <guid>https://chrisoffner.github.io/studynotes/posts/2021/11/deriving-the-optimal-icp-translation/</guid>
            <description>In their paper On the ICP Algorithm the authors Esther Ezra, Micha Sharir, and Alon Efrat write:
 Lemma 2.3 At each iteration $i \ge 2$ of the algorithm, the relative translation vector $\Delta t_i$ satisfies
$$\boldsymbol{\Delta t_i} = \frac{1}{m} \sum_{\boldsymbol{a} \in A} \left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}})\right),$$
where $\boldsymbol{t_j} = \sum_{k=1}^j \boldsymbol{\Delta t_k}.$
Proof Follows using easy algebraic manipulations, based on the obvious equality that follows by construction</description>
            <content type="html"><![CDATA[<p>In their paper <a href="https://dl.acm.org/doi/10.1145/1137856.1137873"><strong>On the ICP Algorithm</strong></a> the authors <a href="https://www.cs.biu.ac.il/en/staff/113">Esther Ezra</a>, <a href="https://www.cs.tau.ac.il/~michas/">Micha Sharir</a>, and <a href="https://www2.cs.arizona.edu/~alon/">Alon Efrat</a> write:</p>
<blockquote>
<p><strong>Lemma 2.3</strong>
At each iteration $i \ge 2$ of the algorithm, the relative translation vector $\Delta t_i$ satisfies</p>
<p>$$\boldsymbol{\Delta t_i} = \frac{1}{m} \sum_{\boldsymbol{a} \in A} \left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}})\right),$$</p>
<p>where $\boldsymbol{t_j} = \sum_{k=1}^j \boldsymbol{\Delta t_k}.$</p>
<p><strong>Proof</strong>
Follows using easy algebraic manipulations, based on the obvious equality that follows by construction</p>
<p>$$\boldsymbol{\Delta t_i} = \frac{1}{m} \left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - (\boldsymbol{a} + \boldsymbol{t_{i-1}})\right)$$</p>
</blockquote>
<p>Unfortunately this supposedly <em>&ldquo;obvious equality&rdquo;</em>  wasn&rsquo;t quite so obvious to me, and neither were the &ldquo;<em>easy</em> algebraic manipulations&rdquo; that would prove the lemma.</p>
<p>Only after working through Cyrill Stachniss' great <a href="https://www.youtube.com/watch?v=dhzLQfDBx2Q"><em>lecture</em></a> on the topic was I able to derive this &ldquo;obvious&rdquo; equality, and then do the algebraic manipulations that would allow me to arrive at this lemma.</p>
<h2 id="getting-to-the-starting-point">Getting to the starting point</h2>
<h3 id="use-local-coordinate-system">Use local coordinate system</h3>
<p>To simplify notation we want to use the local coordinates defined by the point set $B$ and set the origin as the mean of $B$:</p>
<p>$$\boldsymbol{b_0} := \frac{1}{m} \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a + t_{i-1}})$$</p>
<p>Adding and subtracting this $\boldsymbol{b_0}$ to our $\mathrm{RMS}$ cost function does not change it, so that in every iteration $i$ we want to find the $\boldsymbol{\Delta t_i}$ that minimises</p>
<p>$$\mathrm{RMS}(\boldsymbol{\Delta t_i}) = \frac{1}{m}\sum_{a \in A} ||\boldsymbol{N_B(a+t_{i-1})} - \boldsymbol{b_0} - (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{\Delta t_i} + \boldsymbol{b_0}||^2.$$</p>
<h3 id="rewrite-translation-vector">Rewrite translation vector</h3>
<p>We introduce a <em><strong>new variable</strong></em> $\boldsymbol{a_0} := \boldsymbol{b_0} - \boldsymbol{\Delta t_i}$ and can then rewrite the expression of point $\boldsymbol{a}$ translated by the newest translation vector as</p>
<p>$$(\boldsymbol{a} + \boldsymbol{t_{i-1}}) + \boldsymbol{\Delta t_i} - \boldsymbol{b_0} = (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{a_0}.$$</p>
<p>Note that, while the notation of $\boldsymbol{a_0}$ alludes to a semantic meaning similar to that of $\boldsymbol{b_0}$, it is currently simply defined as $\boldsymbol{b_0} - \boldsymbol{\Delta t_i}$, with no obvious connection to the mean of $A$. The connection will show up as we proceed.</p>
<h3 id="minimisation">Minimisation</h3>
<p>Our initially formulated minimisation function</p>
<p>$$\frac{1}{m}\sum_{a \in A} ||\boldsymbol{N_B(a+t_{i-1})} - (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{\Delta t_i}||^2.$$</p>
<p>can therefore be rewritten with $\boldsymbol{a_0}$. Now in order to minimise our function we need to find</p>
<p>$$\argmin_{a_0}\frac{1}{m}\sum_{\boldsymbol{a} \in A} || (\boldsymbol{N_B(a+t_{i-1}}) - \boldsymbol{b_0}) - ((\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{a_0})||^2.$$</p>
<p>Let&rsquo;s simplify the notation in this long expression a bit by defining two new variables</p>
<p>$$\begin{aligned}\boldsymbol{\widetilde{b}} := &amp;N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) \cr \boldsymbol{\widetilde{a}} := &amp;\boldsymbol{a} + \boldsymbol{t_{i-1}}\end{aligned}$$</p>
<p>and use those as we expand our cost function:</p>
<p>$$\begin{aligned}\Phi(\boldsymbol{a_0}) \coloneqq \frac{1}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} &amp;\left[ (\boldsymbol{\widetilde{b}} - \boldsymbol{b_0}) - (\boldsymbol{\widetilde{a}} - \boldsymbol{a_0}) \right]^T\left[ (\boldsymbol{\widetilde{b}} - \boldsymbol{b_0}) - (\boldsymbol{\widetilde{a}} - \boldsymbol{a_0}) \right] \cr = \frac{1}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} &amp;(\boldsymbol{\widetilde{b}} - \boldsymbol{b_0})^T(\boldsymbol{\widetilde{b}} - \boldsymbol{b_0}) &amp;&amp; \text{(constant)} \cr + \frac{1}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} &amp;(\boldsymbol{\widetilde{a}} - \boldsymbol{a_0})^T(\boldsymbol{\widetilde{a}} - \boldsymbol{a_0})&amp;&amp;(*) \cr - \frac{2}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} &amp;(\boldsymbol{\widetilde{b}} - \boldsymbol{b_0})^T(\boldsymbol{\widetilde{a}} - \boldsymbol{a_0}) &amp;&amp; (**)\end{aligned}$$</p>
<p>To find the minimum of this convex function we calculate its first derivative</p>
<p>$$\begin{aligned}
\frac{\mathrm{d} \Phi}{\mathrm{d} \boldsymbol{a_0}} = &amp;-\frac{2}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}}(\boldsymbol{\widetilde{a}} - \boldsymbol{a_0})&amp;&amp;(*)\cr
&amp;+\frac{2}{m}\sum_{\boldsymbol{\widetilde{a}} \in A + \boldsymbol{t_{i-1}}} (\boldsymbol{\widetilde{b}} - \boldsymbol{b_0}) &amp;&amp; (**)
\end{aligned}$$</p>
<p>and set it equal to zero, which simplifies to</p>
<p>$$\sum_{a \in A}((\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{a_0}) = \sum_{a \in A} (\boldsymbol{N_B(a+t_{i-1}}) - \boldsymbol{b_0}).$$</p>
<p>The right side of this equation equals zero because $\boldsymbol{b_0}$ is the mean of $$ all nearest neighbours ${N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}},|, \boldsymbol{a} \in A)}$. Now we can solve for $\boldsymbol{a_0}$:</p>
<p>$$\begin{aligned}0 = &amp;\sum_{a \in A}((\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \boldsymbol{a_0}) \cr = &amp;\sum_{a \in A} (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - \sum a_0 \cr = &amp;\sum_{a \in A} (\boldsymbol{a} + \boldsymbol{t_{i-1}}) - m \cdot \boldsymbol{a_0}\cr\implies \boldsymbol{a_0} = &amp;\frac{1}{m} \sum_{a \in A} (\boldsymbol{a} + \boldsymbol{t_{i-1}}) \end{aligned}$$</p>
<p>which means <strong>the optimal value for $\boldsymbol{a_0}$ is the weighted mean of the points</strong> $\boldsymbol{a} + \boldsymbol{t_{i-1}}$.</p>
<p>Since $\boldsymbol{a_0} = \boldsymbol{b_0} - \boldsymbol{\Delta t_i}$ and $\boldsymbol{b_0} = \frac{1}{m} \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a + t_{i-1}})$ it follows that</p>
<p>$$\begin{aligned}\boldsymbol{\Delta t_i} = &amp; \boldsymbol{b_0} - \boldsymbol{a_0} \cr = &amp;\frac{1}{m} \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a + t_{i-1}}) - \frac{1}{m} \sum_{\boldsymbol{a} \in A} (\boldsymbol{a} + \boldsymbol{t_{i-1}}) \cr = &amp;\frac{1}{m} \sum_{\boldsymbol{a} \in A} \left(N_B(\boldsymbol{a + t_{i-1}}) - (\boldsymbol{a} + \boldsymbol{t_{i-1}})\right) \end{aligned}$$</p>
<p>The optimal translation vector is therefore the mean of differences between points marked as nearest neighbours in $B$ and their corresponding points in $A + \boldsymbol{t_{i-1}}$.</p>
<h2 id="proving-the-lemma">Proving the lemma</h2>
<p>Using this equality we can finally perform the algebraic manipulation that leads us to the lemma, starting with a simple index shift:</p>
<p>$$\begin{aligned}
&amp;&amp;\boldsymbol{\Delta t_{i-1}} &amp;= \frac{1}{m} \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) - (\boldsymbol{a} + \boldsymbol{t_{i-2}}) \cr
\implies&amp;&amp;m\boldsymbol{\Delta t_{i-1}} &amp;=  \sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) - \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-2}}) \cr\implies&amp;&amp;\sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) &amp;= \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-2}}) + m\boldsymbol{\Delta t_{i-1}}\cr\implies&amp;&amp;\sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) &amp;= \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-2}}) + \sum_{\boldsymbol{a} \in A}\boldsymbol{\Delta t_{i-1}}\cr\implies&amp;&amp;\sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) &amp;= \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-2}} + \boldsymbol{\Delta t_{i-1}})\cr\implies&amp;&amp;\sum_{\boldsymbol{a} \in A} N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}}) &amp;= \sum_{\boldsymbol{a} \in A}(\boldsymbol{a} + \boldsymbol{t_{i-1}})\cr\implies&amp;&amp;\frac{1}{m}\sum_{\boldsymbol{a} \in A} \left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) - N_B(\boldsymbol{a} + \boldsymbol{t_{i-2}})\right) &amp;= \underbrace{\frac{1}{m}\sum_{\boldsymbol{a} \in A}\left(N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}}) -(\boldsymbol{a} + \boldsymbol{t_{i-1}})\right)}_{=\boldsymbol{\Delta t_i}}\end{aligned}$$</p>
<p>Thanks to the previously derived equality we know the right side is $\boldsymbol{\Delta t_i}$, and can therefore confirm that the optimal relative translation for every iteration $i \ge 2$ is the mean difference between the current and previous nearest neighbour of each point in $A$.</p>
]]></content>
        </item>
        
        <item>
            <title>Illustrating ICP in one dimension</title>
            <link>https://chrisoffner.github.io/studynotes/posts/2021/11/illustrating-icp-in-one-dimension/</link>
            <pubDate>Sat, 20 Nov 2021 16:35:48 +0100</pubDate>
            
            <guid>https://chrisoffner.github.io/studynotes/posts/2021/11/illustrating-icp-in-one-dimension/</guid>
            <description>To get a better intuition of what the iterative closest point (ICP) algorithm does, let&amp;rsquo;s look at a visual representation of its simplest form ‚Äì the one-dimensional case which is only concerned with translation, but not rotation.
Problem definition Let $A = \{{\boldsymbol{a_1}, &amp;hellip;, \boldsymbol{a_m}\}}, B = \{{\boldsymbol{b_1}, &amp;hellip;, \boldsymbol{b_n} \}}$ be two point sets in $d$-space, in the following illustration $d = 1$, and suppose that the ICP algorithm aligns $A$ to $B$, i.</description>
            <content type="html"><![CDATA[<p>To get a better intuition of what the iterative closest point (ICP) algorithm does, let&rsquo;s look at a visual representation of its simplest form ‚Äì the one-dimensional case which is only concerned with translation, but not rotation.</p>
<h3 id="problem-definition">Problem definition</h3>
<p>Let $A = \{{\boldsymbol{a_1}, &hellip;, \boldsymbol{a_m}\}}, B = \{{\boldsymbol{b_1}, &hellip;, \boldsymbol{b_n} \}}$ be two point sets in $d$-space, in the following illustration $d = 1$, and suppose that the ICP algorithm aligns $A$ to $B$, i.e. $B$ is fixed and $A$ is translated to best fit $B$.</p>
<p>The algorithm repeatedly iterates over its two steps:</p>
<ol>
<li>
<p>For each (translated) point $\boldsymbol{a} + \boldsymbol{t_{i-1}} \ in A + \boldsymbol{t_{i-1}}$ it registers a correspondence to its nearest neighbour $N_B(\boldsymbol{a} + \boldsymbol{t_{i-1}})$ in $B$.</p>
</li>
<li>
<p>Based on these point correspondences, it then computes a relative translation vector $\boldsymbol{\Delta t_i}$ that minimises a cost function $\Phi$ and translates $A + \boldsymbol{t_{i-1}}$ by that vector.</p>
</li>
</ol>
<p>The algorithm terminates once $\boldsymbol{\Delta t_i} = 0$.</p>
<h3 id="illustration">Illustration</h3>
<p>Let&rsquo;s now look at a visual representation of a full run of the ICP algorithm in one dimension. For better illustration, the point sets $A$ (red) and $B$ (blue) have been offset vertically, but are to be understood as both lying on the same one-dimensional number line $\R$. The following example uses the <em><a href="https://en.wikipedia.org/wiki/Root_mean_square">root mean square</a></em> cost function</p>
<p>$$\Phi(\boldsymbol{\Delta t_i}) = \mathrm{RMS}(\boldsymbol{\Delta t_i}) = \frac{1}{m}\sum_{a \in A}|| (a + t_i + \boldsymbol{\Delta t_i}) - N_B(a + t_{i-1}) ||^2$$</p>
<p>where $||\cdot||$ is the Euclidean norm and $m = |A|$.</p>
<p>The notation largely follows the conventions laid out in <em><a href="https://dl.acm.org/doi/10.1145/1137856.1137873">On the ICP Algorithm</a></em> by Esther Ezra, Micha Sharir, and Alon Efrat.</p>
<h4 id="iteration-1">Iteration 1</h4>
<p>We begin with $A = \{{ 16, 17, 25, 26 \}}$ and $B = \{{1, 2, 10, 11\}}$. It is easy to see that in this first iteration, all points of $A$ will be assigned $\boldsymbol{b_4} = 11$ as their nearest neighbour, and will therefore be &ldquo;pulled&rdquo; strongly to the left.</p>
<p><img src="/images/icp_graph_01.svg" alt="icp_graph_01.svg"></p>
<h4 id="iteration-2">Iteration 2</h4>
<p>After the first translation by $\boldsymbol{\Delta t_1} = -10$, something interesting happens. The two left-most points in the now translated red set, $A + \boldsymbol{t_1}$, have moved so far that they get assigned new nearest neighbours. Now the second red point $a_2 +  \boldsymbol{t_1}$ pulls to its right, and the resulting optimal translation vector $\boldsymbol{\Delta t_2} = -2.5$ is already much shorter than before.</p>
<p><img src="/images/icp_graph_02.svg" alt="icp_graph_02.svg"></p>
<h4 id="iteration-3">Iteration 3</h4>
<p>In the next iteration, the second red point&rsquo;s corresponding point changes again, causing it to join the others again in &ldquo;pulling&rdquo; further to the left.</p>
<p><img src="/images/icp_graph_03.svg" alt="icp_graph_03.svg"></p>
<h4 id="iteration-4">Iteration 4</h4>
<p>Now the first and third red point get new nearest neighbours in $B$, resulting in one final slight tug to the left.</p>
<p><img src="/images/icp_graph_04.svg" alt="icp_graph_04.svg"></p>
<h3 id="iteration-5">Iteration 5</h3>
<p>In the end, $A + \boldsymbol{t_4}$ rests in the optimal position so that $\boldsymbol{\Delta t_5} = 0$ and the algorithm terminates.</p>
<p><img src="/images/icp_graph_05.svg" alt="icp_graph_05.svg"></p>
<p>Next up we will derive the optimal relative translation vector $\boldsymbol{\Delta t_i}$ at every step.</p>
]]></content>
        </item>
        
    </channel>
</rss>
